{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c49fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import pathlib\n",
    "import random\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04cfdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "ENG_VOCAB_SIZE = 15000\n",
    "SPA_VOCAB_SIZE = 15000\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70dbd8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you still thinking about applying to harvard? ¿sigues pensando en postular a harvard?\n",
      "do you want to eat prawns? ¿quieres comer camarones?\n",
      "i had no difficulty in finding his house. no tuve problemas para encontrar su casa.\n"
     ]
    }
   ],
   "source": [
    "text_file = keras.utils.get_file(fname=\"spa-eng.zip\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\", extract=True,) # WindowsPath('C:/Users/13900K/.keras/datasets/spa-eng/spa.txt')\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
    "text_pairs = []\n",
    "with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        eng, spa = line.strip().split(\"\\t\")\n",
    "        eng = eng.lower()\n",
    "        spa = spa.lower()\n",
    "        if eng and spa:\n",
    "            text_pairs.append((eng, spa))\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples+num_val_samples:]\n",
    "for e,s in train_pairs[:3]:\n",
    "    print(e,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31ae674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", ',']\n",
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', '&', \"'\"]\n",
      "English sentence:  are you still thinking about applying to harvard?\n",
      "Tokens:  tf.Tensor([  83   64  211  591  115 2656  148   63 2193   25], shape=(10,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b'are you still thinking about applying to harvard ?', shape=(), dtype=string)\n",
      "\n",
      "Spanish sentence:  ¿sigues pensando en postular a harvard?\n",
      "Tokens:  tf.Tensor([  62 3872  704   82   45  469  905 2285   30 2710   29], shape=(11,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b'\\xc2\\xbf sigues pensando en postular a harvard ?', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(word_piece_ds.batch(1000).prefetch(2), vocabulary_size=vocab_size, reserved_tokens=reserved_tokens)\n",
    "    return vocab\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
    "spa_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)\n",
    "eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(vocabulary=eng_vocab, lowercase=False)\n",
    "spa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(vocabulary=spa_vocab, lowercase=False)\n",
    "\n",
    "print(eng_vocab[:10])\n",
    "print(spa_vocab[:10])\n",
    "\n",
    "eng_input_ex = text_pairs[0][0]\n",
    "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
    "print(\"English sentence: \", eng_input_ex)\n",
    "print(\"Tokens: \", eng_tokens_ex)\n",
    "print( \"Recovered text after detokenizing: \",  eng_tokenizer.detokenize(eng_tokens_ex), )\n",
    "\n",
    "print()\n",
    "\n",
    "spa_input_ex = text_pairs[0][1]\n",
    "spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n",
    "print(\"Spanish sentence: \", spa_input_ex)\n",
    "print(\"Tokens: \", spa_tokens_ex)\n",
    "print(  \"Recovered text after detokenizing: \",  spa_tokenizer.detokenize(spa_tokens_ex),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ea2f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  6  7  2]\n",
      " [ 1  8  9 10  2]]\n",
      "[[ 1  5  6  7  2  0  0  0  0  0]\n",
      " [ 1  8  9 10 11 12 13 14  2  0]]\n"
     ]
    }
   ],
   "source": [
    "# StartEndPacker 会先将 tokens 加入 [START] 和 [END] 两个 token \n",
    "# 长度不够的用 [PAD] 补齐长度为 sequence_length ，直接全部返回\n",
    "# 长度超了就将中间的代表文本的 tokens 截断，但是必须要保留 [START] 和 [END] 两个 token .最终加起来长度是 sequence_length\n",
    "import numpy as np\n",
    "inputs = [[5, 6, 7], [8, 9, 10, 11, 12, 13, 14]]\n",
    "start_end_packer = keras_nlp.layers.StartEndPacker( sequence_length=5, start_value=1, end_value=2,)\n",
    "outputs = start_end_packer(inputs)\n",
    "print(np.array(outputs))\n",
    "start_end_packer = keras_nlp.layers.StartEndPacker( sequence_length=10, start_value=1, end_value=2,)\n",
    "outputs = start_end_packer(inputs)\n",
    "print(np.array(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ef47d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  79  289  160   26  650   11    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  99    8   44  127  221   80    8  105  132   63   73   70   11    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  67   26  418 1449   86 1106 1507  418 3400   69  956   11    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(3, 40), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[   2   98  146  311   84  774   15    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   76  629  645  383   30  124  111   15    3    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   30  165 1488  451  103   32  315 1431   96 1305   15    3    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(3, 40), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  98  146  311   84  774   15    3    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  76  629  645  383   30  124  111   15    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  30  165 1488  451  103   32  315 1431   96 1305   15    3    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(3, 40), dtype=int32)\n",
      "inputs[\"encoder_inputs\"].shape: (64, 40)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 40)\n",
      "targets.shape: (64, 40)\n"
     ]
    }
   ],
   "source": [
    "def process_batch(eng, spa):\n",
    "    batch_size = tf.shape(spa)[0]\n",
    "    eng = eng_tokenizer(eng)\n",
    "    spa = spa_tokenizer(spa)\n",
    "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(sequence_length=MAX_SEQUENCE_LENGTH, pad_value=eng_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    eng = eng_start_end_packer(eng)\n",
    "    spa_start_end_packer = keras_nlp.layers.StartEndPacker(sequence_length=MAX_SEQUENCE_LENGTH+1, start_value=spa_tokenizer.token_to_id(\"[START]\"), end_value=spa_tokenizer.token_to_id(\"[END]\"), pad_value=spa_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    spa = spa_start_end_packer(spa)\n",
    "    return{\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1]},spa[:,1:]\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts)).batch(BATCH_SIZE).map(process_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(inputs[\"encoder_inputs\"][:3])\n",
    "    print(inputs[\"decoder_inputs\"][:3])\n",
    "    print(targets[:3])\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a016b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1302/1302 [==============================] - 21s 15ms/step - loss: 1.1901 - accuracy: 0.8216 - val_loss: 0.9378 - val_accuracy: 0.8340\n",
      "Epoch 2/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.8949 - accuracy: 0.8415 - val_loss: 0.8080 - val_accuracy: 0.8517\n",
      "Epoch 3/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.7647 - accuracy: 0.8590 - val_loss: 0.7144 - val_accuracy: 0.8639\n",
      "Epoch 4/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.6660 - accuracy: 0.8724 - val_loss: 0.6292 - val_accuracy: 0.8775\n",
      "Epoch 5/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.5801 - accuracy: 0.8855 - val_loss: 0.5625 - val_accuracy: 0.8891\n",
      "Epoch 6/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.5110 - accuracy: 0.8967 - val_loss: 0.5191 - val_accuracy: 0.8965\n",
      "Epoch 7/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.4618 - accuracy: 0.9047 - val_loss: 0.4935 - val_accuracy: 0.9015\n",
      "Epoch 8/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.4255 - accuracy: 0.9105 - val_loss: 0.4798 - val_accuracy: 0.9042\n",
      "Epoch 9/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.3965 - accuracy: 0.9154 - val_loss: 0.4715 - val_accuracy: 0.9068\n",
      "Epoch 10/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.3726 - accuracy: 0.9197 - val_loss: 0.4675 - val_accuracy: 0.9087\n",
      "Epoch 11/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.3524 - accuracy: 0.9230 - val_loss: 0.4602 - val_accuracy: 0.9104\n",
      "Epoch 12/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.3347 - accuracy: 0.9262 - val_loss: 0.4566 - val_accuracy: 0.9111\n",
      "Epoch 13/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.3195 - accuracy: 0.9288 - val_loss: 0.4573 - val_accuracy: 0.9113\n",
      "Epoch 14/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.3052 - accuracy: 0.9314 - val_loss: 0.4606 - val_accuracy: 0.9113\n",
      "Epoch 15/100\n",
      "1302/1302 [==============================] - 20s 15ms/step - loss: 0.2923 - accuracy: 0.9336 - val_loss: 0.4672 - val_accuracy: 0.9122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19bd616c340>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder\n",
    "encoder_inputs = tf.keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(vocabulary_size=ENG_VOCAB_SIZE, sequence_length=MAX_SEQUENCE_LENGTH, embedding_dim=EMBED_DIM)(encoder_inputs)\n",
    "encoder_output = keras_nlp.layers.TransformerEncoder(intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS)(inputs=x)\n",
    "encoder = tf.keras.Model(encoder_inputs, encoder_output)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = tf.keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(vocabulary_size=SPA_VOCAB_SIZE, sequence_length=MAX_SEQUENCE_LENGTH, embedding_dim=EMBED_DIM)(decoder_inputs)\n",
    "x = keras_nlp.layers.TransformerDecoder(intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS)(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = tf.keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = tf.keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_output])\n",
    "\n",
    "# transformer\n",
    "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
    "transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38939457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = len(input_sentences)\n",
    "    encoder_input_tokens = eng_tokenizer(input_sentences)\n",
    "    encoder_input_tokens = encoder_input_tokens.to_tensor(shape=[None, MAX_SEQUENCE_LENGTH], default_value=eng_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    start = tf.fill([batch_size, 1], value=spa_tokenizer.token_to_id(\"[START]\"))\n",
    "    end = spa_tokenizer.token_to_id(\"[END]\")\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "    output_array = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "    for i in tf.range(MAX_SEQUENCE_LENGTH):\n",
    "        output = tf.transpose(output_array.stack(), perm=[1, 0, 2])\n",
    "        predictions = transformer([encoder_input_tokens, tf.squeeze(output, axis=-1)])  # [B,1,V]\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.argmax(predictions, axis=-1,output_type=tf.int32)\n",
    "        done |= predicted_id == end\n",
    "        predicted_id = tf.where(done, tf.constant(0, dtype=tf.int32), predicted_id)  # [B, 1]\n",
    "        output_array = output_array.write(i+1, predicted_id)\n",
    "        if tf.reduce_all(done):\n",
    "            break\n",
    "    output = tf.transpose(output_array.stack(), perm=[1, 0, 2])\n",
    "    output = tf.squeeze(output, axis=-1)  # Remove the last dimension\n",
    "    texts =  spa_tokenizer.detokenize(output)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2e03a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文： tom worked on a farm last summer.\n",
      "翻译： tom trabajó en un gran granja .\n",
      "\n",
      "英文： you shouldn't spend more money than you earn.\n",
      "翻译： no deberías gastar más dinero como si tienes .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "input_sentence = random.choices(test_eng_texts, k=2)\n",
    "translated = decode_sequences(input_sentence)\n",
    "for i,text in enumerate(input_sentence):\n",
    "    print(\"英文：\",text.strip())\n",
    "    print(\"翻译：\",translated.numpy()[i].decode(\"utf-8\").replace(\"[START]\", \"\").replace(\"[PAD]\", \"\").replace(\"[END]\", \"\").strip()) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4171e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.5684036>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.5268915>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.54191023>}\n",
      "ROUGE-2 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.3932744>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.35842922>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.3713763>}\n"
     ]
    }
   ],
   "source": [
    "rouge_1 = keras_nlp.metrics.RougeN(order=1)\n",
    "rouge_2 = keras_nlp.metrics.RougeN(order=2)\n",
    "for test_pair in test_pairs[:30]:\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "    translated_sentence = decode_sequences([input_sentence])\n",
    "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
    "    translated_sentence = translated_sentence.replace(\"[PAD]\", \"\").replace(\"[START]\", \"\").replace(\"[END]\", \"\").strip()\n",
    "    rouge_1(reference_sentence, translated_sentence)\n",
    "    rouge_2(reference_sentence, translated_sentence)\n",
    "\n",
    "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
    "print(\"ROUGE-2 Score: \", rouge_2.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1G-4.3G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27eeec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding_2  (None, None, 256)   3850240     ['encoder_inputs[0][0]']         \n",
      "  (TokenAndPositionEmbedding)                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_1 (Transfo  (None, None, 256)   1315072     ['token_and_position_embedding_2[\n",
      " rmerEncoder)                                                    0][0]']                          \n",
      "                                                                                                  \n",
      " model_3 (Functional)           (None, None, 15000)  9283992     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,449,304\n",
      "Trainable params: 14,449,304\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a14db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2.10.0-py-3.10",
   "language": "python",
   "name": "tf-gpu-2.10.0-py-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
