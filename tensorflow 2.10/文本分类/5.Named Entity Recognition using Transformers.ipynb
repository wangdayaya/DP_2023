{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7bbc2b8-2d8d-4b14-80b8-c3c153130144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from conlleval import evaluate\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a8f155d-d7c8-4652-99d5-925c6b1fa5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(maxlen, embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(0, maxlen, 1)\n",
    "        position_embeddings=  self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings\n",
    "\n",
    "class NERModel(tf.keras.Model):\n",
    "    def __init__(self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.ff = tf.keras.layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "        self.ff_final = tf.keras.layers.Dense(num_tags, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training)\n",
    "        x = self.ff_final(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "962a8446-f95b-4b76-ba48-ed5feb9565dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要是处理数据保存早本地\n",
    "# conll_data = load_dataset(\"conll2003\")\n",
    "# def export_to_file(export_file_path, data):\n",
    "#     with open(export_file_path, \"w\") as f:\n",
    "#         for record in data:\n",
    "#             ner_tags = record[\"ner_tags\"]\n",
    "#             tokens = record[\"tokens\"]\n",
    "#             if len(tokens) > 0:\n",
    "#                 f.write(\n",
    "#                     str(len(tokens))\n",
    "#                     + \"\\t\"\n",
    "#                     + \"\\t\".join(tokens)\n",
    "#                     + \"\\t\"\n",
    "#                     + \"\\t\".join(map(str, ner_tags))\n",
    "#                     + \"\\n\"\n",
    "#                 )\n",
    "\n",
    "\n",
    "# os.mkdir(\"data\")\n",
    "# export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
    "# export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d53f8a6-3203-4e9a-b7ec-e932d3c42dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (C:/Users/13900K/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1000.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0']\n",
      "tf.Tensor(\n",
      "[[  988 10950   204   628     6  3938   215  5773     2     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0]\n",
      " [  773  1871     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0]], shape=(2, 47), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[4 1 8 1 1 1 8 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0]], shape=(2, 47), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a,b]) for a,b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    return dict(zip(range(0,len(all_labels)+1), all_labels))\n",
    "    \n",
    "conll_data = load_dataset(\"conll2003\")\n",
    "mapping = make_tag_lookup_table()\n",
    "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "counter = Counter(all_tokens_array)   # 21009 个\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "lookup_layer = tf.keras.layers.StringLookup(vocabulary=vocabulary)\n",
    "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
    "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")\n",
    "print()\n",
    "print(list(train_data.take(1).as_numpy_iterator()))\n",
    "\n",
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(record, sep=\"\\t\")\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
    "    tokens = record[1:length+1]\n",
    "    tags = record[length+1:]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    tokens = lookup_layer(tokens)\n",
    "    return tokens, tags\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = train_data.map(map_record_to_training_data).padded_batch(batch_size)\n",
    "val_ds = val_data.map(map_record_to_training_data).padded_batch(batch_size)\n",
    "for tokens, tags in train_ds.take(1):\n",
    "    print(tokens[:2])\n",
    "    print(tags[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "747c0e70-0bac-49f7-801b-2c838fc5ee9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "439/439 [==============================] - 3s 6ms/step - loss: 0.6048 - custom_accuracy: 0.8599 - val_loss: 0.3492 - val_custom_accuracy: 0.9092\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 2s 5ms/step - loss: 0.2491 - custom_accuracy: 0.9304 - val_loss: 0.2777 - val_custom_accuracy: 0.9261\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 2s 5ms/step - loss: 0.1548 - custom_accuracy: 0.9537 - val_loss: 0.2549 - val_custom_accuracy: 0.9310\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 2s 5ms/step - loss: 0.1177 - custom_accuracy: 0.9627 - val_loss: 0.2407 - val_custom_accuracy: 0.9338\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 2s 5ms/step - loss: 0.0939 - custom_accuracy: 0.9685 - val_loss: 0.2405 - val_custom_accuracy: 0.9333\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 2s 5ms/step - loss: 0.0772 - custom_accuracy: 0.9732 - val_loss: 0.2461 - val_custom_accuracy: 0.9322\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 2s 5ms/step - loss: 0.0654 - custom_accuracy: 0.9765 - val_loss: 0.2522 - val_custom_accuracy: 0.9335\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 2s 5ms/step - loss: 0.0570 - custom_accuracy: 0.9786 - val_loss: 0.2716 - val_custom_accuracy: 0.9363\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 2s 5ms/step - loss: 0.0508 - custom_accuracy: 0.9809 - val_loss: 0.3165 - val_custom_accuracy: 0.9338\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 2s 5ms/step - loss: 0.0453 - custom_accuracy: 0.9827 - val_loss: 0.2934 - val_custom_accuracy: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fda5c69090>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomNonPaddingTokenLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super(CustomNonPaddingTokenLoss, self).__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true!=0), dtype=tf.float32)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "class CustomNonPaddingTokenAcc(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='custom_accuracy', **kwargs):\n",
    "        super(CustomNonPaddingTokenAcc, self).__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        y_pred = tf.cast(y_pred, dtype=y_true.dtype)\n",
    "        y_pred = tf.cast(y_pred, dtype=y_true.dtype)\n",
    "        match = tf.cast(tf.equal(y_pred, y_true), dtype=tf.float32)\n",
    "        mask = tf.cast((y_true!=0),  dtype=tf.float32)\n",
    "        self.total.assign_add(tf.reduce_sum(match))\n",
    "        self.count.assign_add(tf.reduce_sum(mask))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total.assign(0)\n",
    "        self.count.assign(0)\n",
    "\n",
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model.compile(optimizer=\"adam\", loss=CustomNonPaddingTokenLoss(), metrics=[CustomNonPaddingTokenAcc()])\n",
    "ner_model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=[tf.keras.callbacks.EarlyStopping(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd857a-6a72-4099-95b4-d6e42846417a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a9c6fc35-00ef-43ba-b7c1-4334f32f9dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "预测 tokens：['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "实际 tokens：['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "预测 tokens：['B-LOC', 'O']\n",
      "实际 tokens：['B-LOC', 'O']\n",
      "\n",
      "预测 tokens：['B-MISC', 'B-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "实际 tokens：['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "预测 tokens：['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "实际 tokens：['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "\n",
      "预测 tokens：['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
      "实际 tokens：['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tokens, tags in val_ds.take(1):\n",
    "    break\n",
    "n = 5\n",
    "lens = tf.reduce_sum(tf.cast(tf.not_equal(tags, 0), tf.int32), axis=-1).numpy()\n",
    "pred = tf.argmax(ner_model.predict(tokens[:n]), axis=-1).numpy()\n",
    "true = tags[:n].numpy()\n",
    "for i,(a,b) in enumerate(zip(pred, true)):\n",
    "    print(f\"预测 tokens：{[mapping[item] for item in a][:lens[i]]}\")\n",
    "    print(f\"实际 tokens：{[mapping[item] for item in b][:lens[i]]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41acbd1-0bb8-4239-9b51-b664a993f3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b13e68-9090-452c-a157-8b7835222394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195711b0-7a89-4322-b33d-961ce50028ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2.10.0-py-3.10",
   "language": "python",
   "name": "tf-gpu-2.10.0-py-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
