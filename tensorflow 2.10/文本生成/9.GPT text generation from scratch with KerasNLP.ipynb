{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195f46fe-d678-4ce5-a0f3-a281475a1bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "import tensorflow as tf\n",
    "\n",
    "for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "786c735b-da14-49fc-984b-986b50e22977",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MIN_STRING_LEN = 512\n",
    "SEQ_LEN = 128\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 128\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 5000\n",
    "EPOCHS = 5\n",
    "NUM_TOKENS_TO_GENERATE = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2199da8b-b8d0-46cb-87e3-689bda4147d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.get_file(origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\", extract=True, )\n",
    "dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "raw_train_ds = tf_data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\").filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN).batch(BATCH_SIZE).shuffle(buffer_size = 256)\n",
    "raw_val_ds = tf_data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\").filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN).batch(BATCH_SIZE)\n",
    "# 一个单词列表，并且前三个分别是 '[PAD]','[UNK]','[BOS]'\n",
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(raw_train_ds, vocabulary_size=VOCAB_SIZE, lowercase=True, reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"])\n",
    "# 根据 vocab 可以给句子进行分词并转换成对应的 id ，id 长度为 SEQ_LEN ，不足的用 0 补齐，超出的截断\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(vocabulary=vocab, sequence_length=SEQ_LEN, lowercase=True)\n",
    "# 在 id 序列的开始加上 [BOS] 对应的 id ，最后的 id 序列长度为 SEQ_LEN ，不足的用 0 补齐，超出的截断\n",
    "start_packer = keras_nlp.layers.StartEndPacker(sequence_length=SEQ_LEN, start_value=tokenizer.token_to_id(\"[BOS]\"))\n",
    "def preprocess(inputs):\n",
    "    outputs = tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(tf_data.AUTOTUNE)\n",
    "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4dce97a-9526-487c-a461-df0caaf52c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2445/2445 [==============================] - 50s 18ms/step - loss: 4.6896 - perplexity: 109.4358 - val_loss: 4.1836 - val_perplexity: 67.3291\n",
      "Epoch 2/5\n",
      "2445/2445 [==============================] - 48s 18ms/step - loss: 4.1402 - perplexity: 63.1481 - val_loss: 4.0363 - val_perplexity: 58.0357\n",
      "Epoch 3/5\n",
      "2445/2445 [==============================] - 49s 18ms/step - loss: 4.0167 - perplexity: 55.8026 - val_loss: 4.0056 - val_perplexity: 56.2995\n",
      "Epoch 4/5\n",
      "2445/2445 [==============================] - 47s 18ms/step - loss: 3.9478 - perplexity: 52.0794 - val_loss: 3.9524 - val_perplexity: 53.3516\n",
      "Epoch 5/5\n",
      "2445/2445 [==============================] - 50s 19ms/step - loss: 3.9020 - perplexity: 49.7459 - val_loss: 3.9207 - val_perplexity: 51.6600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282840929e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(vocabulary_size=VOCAB_SIZE, sequence_length=SEQ_LEN, embedding_dim=EMBED_DIM, mask_zero=True)\n",
    "x = embedding_layer(inputs)\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(num_heads=NUM_HEADS, intermediate_dim=FEED_FORWARD_DIM)\n",
    "    x = decoder_layer(x)\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d4191d96-9a24-4960-b34a-3a6cc33fc001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[   2, 4608,  124,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens = start_packer(tokenizer([\"today is \"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0a498181-f492-4ad1-bd9f-e7f14c5ce56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next(prompt, cache, index):\n",
    "    logits = model(prompt)[:, index-1, :]\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "32804102-8180-481e-aecd-9c5f118d2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy search generated text: \n",
      "[b'[BOS] today is the way to the westward , and the sparks of the sea , and the sea - shore of the sea , and the sea - shore of the sea , and the sea coasts of america , and the sea coasts of america , and the sea coasts of america , and the sea coasts of america , and the sea coasts of america , and the sea coasts of america , and the sea coasts of america , and the sea coasts of america , and the sea coasts of america , and the sea coasts of america , and the sea coasts of america , and the']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.GreedySampler()\n",
    "output_tokens = sampler(next=next, prompt=prompt_tokens, index=3,) # 开始采样的 `prompt` 的第一个索引。通常将其设置为 `prompt` 中最短非填充序列的长度。  \n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "# 贪婪搜索一开始是有意义的，但很快就开始重复。这是文本生成的一个常见问题，可以通过稍后介绍的一些概率文本生成实用程序来解决！\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "98963dc8-68b3-4d37-a411-a9e0e4c37f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search generated text: \n",
      "[b\"[BOS] today is one of the most apprehension of the chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer ' s chatterer '\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)  # num_beams=1贪婪搜索相同\n",
    "output_tokens = sampler( next=next, prompt=prompt_tokens, index=3, )\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "# 与贪婪搜索类似，束搜索很快开始重复，因为它仍然是一种确定性方法。\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e690d073-240d-46d2-849a-220d61d2b8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random search generated text: \n",
      "[b\"[BOS] today is the air of great husticity of need . haru b rivers made the play all over . this was very strange to hang alone in the heart of the northern power of being the most garagel , and the squirrel panther ' s print of the youthful and power , but a number of chatterer lines of the giants and he kept hidden from the magic whizard and duration . to relyanted mole quite that the insondence upon the brow of the treeping shadow witch ! if he were to go worship ;\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 在每个时间步骤中，它使用模型提供的 softmax 概率对下一个标记进行采样。\n",
    "sampler = keras_nlp.samplers.RandomSampler()\n",
    "output_tokens = sampler(  next=next, prompt=prompt_tokens, index=3, )\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "# 没有重复！但是，使用随机搜索时，我们可能会看到一些无意义的单词出现，因为词汇表中的任何单词都有机会通过这种采样方法出现。\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "613a753f-84b7-4978-84eb-c0398deb6156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-K search generated text: \n",
      "[b'[BOS] today is one of these , and the other , a little boy who has been left behind and saw his wife coming up at her . he is a switch in the chatterer . the man , who is one of the most powerful scar , and he has never been able to get a chuckle in his hand and a man of great bluff . he has a good shot , too , and he has to make a strong scale . but he has the strength of his body , for he is a man . it has not been for a long time , and he has been in']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 与随机搜索类似，我们从模型提供的概率分布中抽取下一个标记。\n",
    "# 唯一的区别是，在这里，我们选择最k有可能的标记，并在抽样之前将概率质量分布在它们上面。\n",
    "# 这样，我们就不会从低概率的标记中抽样，因此我们会得到更少的无意义的单词！\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
    "output_tokens = sampler(  next=next, prompt=prompt_tokens,  index=3, )\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "40d39aff-b347-4a6d-8e2e-a09d695d79a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-P search generated text: \n",
      "[b'[BOS] today is in the same place , and he had no more quarrels with his brother , but the political men were in the woods . the only time he tried to make a new kind of fire , but he was a good man , and he said : \" i am a good fellow , \" and he was not only the good fellow , but it is so i am not much sorry for him . it was the bad fellow , who was to be more to keep up the tree in his hand . \" he is very smart . \" but , when he came to the house of his wife , he was a man ,']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 top-k 搜索，数量k是固定的，这意味着它对任何概率分布都选择相同数量的标记。\n",
    "# 考虑两种情况，一种是概率质量集中在 2 个单词上，另一种是概率质量均匀集中在 10 个单词上。我们应该选择k=2还是k=10？这里不适合 top-k 。\n",
    "# 通过设置p=0.9，如果 90% 的概率集中在前 2 个 token 上，我们可以筛选出前 2 个 token 进行采样。如果 90% 的概率分布在 10 个 token 上，它同样会筛选出前 10 个 token 进行采样。\n",
    "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler( next=next,  prompt=prompt_tokens,  index=3, )\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1a89d-a205-49ef-84ae-6fdfa342648b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2.10.0-py-3.10",
   "language": "python",
   "name": "tf-gpu-2.10.0-py-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
