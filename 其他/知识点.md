# learning

# trl
一组工具来通过强化学习训练 Transformer   https://github.com/huggingface/trl ，https://github.com/lansinuote/Simple_TRL/tree/main


![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ada7894aca6d4d18aaf8a0b2dc34da09~tplv-k3u1fbpfcp-watermark.image?)

Fine-tuning a language model via PPO consists of roughly three steps:

1.  **Rollout**: The language model generates a response or continuation based on query which could be the start of a sentence.
1.  **Evaluation**: The query and response are evaluated with a function, model, human feedback or some combination of them. The important thing is that this process should yield a scalar value for each query/response pair.
1.  **Optimization**: This is the most complex part. In the optimisation step the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the model that is trained and a reference model, which is usually the pre-trained model before fine-tuning. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses don't deviate too far from the reference language model. The active language model is then trained with PPO.

# denoising-diffusion-pytorch 实现

https://github.com/lucidrains/denoising-diffusion-pytorch
#  stable-diffusion 原理

https://github.com/CompVis/stable-diffusion#reference-sampling-script
# 强化学习

强化学习是机器通过与环境交互来实现目标的一种计算方法。这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的累积奖励的期望。

- https://www.bilibili.com/video/BV1Ge4y1i7L6/  ， https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/
- https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/
    
![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/106f651fe919444b9389dc238facd071~tplv-k3u1fbpfcp-watermark.image?)
    
    
1. 折扣回报 Discounted Return ：从第t时刻状态开始，直到终止状态时，所有奖励的折扣衰减之和。依赖于 t 时刻及之后每个时刻的的每个动作 a<sub>i</sub> 和每个状态 s<sub>i</sub>，公式如下：


![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c533a68e3e6b42debc9809119c378227~tplv-k3u1fbpfcp-watermark.image?)


2. 由折扣回报公式我们可以看出，每个时刻 t 的动作 a<sub>i</sub> 和状态 s<sub>i</sub> 都是随机的，所以状态折扣回报中的随机性来源于两方面，一方面是状态转移函数对状态的采样，另一方面是策略函数对动作的采样。


3. 动作价值函数：给定策略的情况下，对当前状态 s 执行动作 a 得到的期望回报


    ![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c1b86ac07316419db84f8c3c66ad5d07~tplv-k3u1fbpfcp-watermark.image?)

    另外还有一个最优动作价值函数，表示无论使用什么策略函数，在状态 s<sub>i</sub> 下采取动作 a<sub>i</sub> 的结果都不可能比 Q<sup>⋆</sup>(s<sub>i</sub>.a<sub>i</sub>) 的值大。DQN 就是一个用来近似这个函数的神经网络模型。


    ![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b704986c87ed450c95e01cf5ba7e87c3~tplv-k3u1fbpfcp-watermark.image?)
 

3. 未来期望奖励（价值函数）：一个状态 s 的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）。所有状态的价值就组成了价值函数（value function），价值函数的输入为某个状态，输出为这个状态的价值。 
    
4. 状态价值函数：定义为从状态 s 出发遵循给定的策略能获得的期望回报，只依赖于当前的状态，后面的状态和动作都被积分积掉了。


    ![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a20094bd57f540cca47284f40d5440d1~tplv-k3u1fbpfcp-watermark.image?)




    状态价值函数和动作价值函数之间的关系：在使用策略情况下，状态 S 的价值等于在该状态下基于策略采取所有动作的概率与相应的动作价值相乘再求和的结果：

    ![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/91c400476d034bb19b3a691022b71301~tplv-k3u1fbpfcp-watermark.image?)

    连起来就是这样的转换公式。
    ![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/13cf0820b3354befa2744513b705af37~tplv-k3u1fbpfcp-watermark.image?)


5. 策略价值函数
6. Actor-Critic
7. 策略梯度、求导
8. DDPG：连续动作
9. Proximal Policy Optimization（PPO） ：https://keras.io/examples/rl/ppo_cartpole/
10. Q-learning：离散动作，最优动作价值函数，训练 DQN ，推荐的操作能够最大化未来奖励期望
11. Deep Q-Learning、 Deep Q-Network、最优状态价值函数：此方法被认为是“Off-Policy”方法， “Q 值”是预期最高长期奖励的加权和。 Q-Learning Agent 学习执行其任务，以便推荐选择输出层中预测的 Q 值中较大的一个来选择操作。建立一个目标模型，权重每 10000 步更新一次，为了计算目标 Q 值是稳定的。
12. SARSA：动作价值函数，训练 critic
13. REINSORCE
14. 蒙特卡洛算法：是一种基于概率统计的数值计算方法，通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值，单纯的估计结果是错的，但是利用大数定理使用随机样本估算真实值，低精度预测结果能大幅度减少计算量
15. ON-POLICY



![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fcd83b1ce16546709f4251cd17cf7b0b~tplv-k3u1fbpfcp-watermark.image?)

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c6354d78e3b242a4b2f3871ecde88088~tplv-k3u1fbpfcp-watermark.image?)

16. OFF-POLICY
17. agent 智能体
18. 随机性：每一轮都伴随着两方面的随机性：一是智能体决策的动作的随机性，二是环境基于当前状态和智能体动作来采样下一刻状态的随机性
19. 动作加噪： OU 加噪算法，DDPG 中先得到动作然后加噪，进行上下限裁剪
20. epsilon-greedy for exploration
21. 折扣率: 又叫折扣因子，的取值范围为 0-1 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的更关注长期的累计奖励，接近 0 的更考虑短期奖励。
22. TD 算法：one-step、multi-step
23. target 模型、延迟更新参数、避免高估或者低估、slow-learning target networks
24. 经验回放
25. 监督学习和强化学习的区别，有监督学习是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数;强化学习任务的最终优化目标是最大化智能体策略在和动态环境交互过程中的价值
        
        
        
        
        
        

# 2023 年大模型总结

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fcd2f58f0f014db4ad2d9823ffa8d899~tplv-k3u1fbpfcp-watermark.image?)

# 前言内容

1. PEFT、SFT、RM、RLHF、大模型微调、、DeepSpeed、Megatron-LM、解决模型幻觉、知识对齐、可控生成 
2. 熟悉业界领先的 LLM 系列，包括但不限于 GPT、LLaMA、GLM、Bloom 等，LLM 预训练和优化。
3. Stable Diffusion、SD、ControlNet、diffusion、图片编辑、文生图、图生文、CLIP、SAM、VAE、U-Net

# 图像生成

1. AE ：encoder + decoder ，一般使用的是训练好的 encoder ，用于图片的压缩、特征提取、去噪等
2. VAE

    重参数化技巧:核心思想是通过引入噪声项 epsilon，将采样的过程拆分成两步。首先从标准正态分布中采样噪声 epsilon，然后使用潜在变量的均值和方差进行线性变换。这个过程使得采样过程是可微的，从而可以通过反向传播进行训练。 数学上，这个过程可以表示为：
  
    ![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2988bf91e16e4b879906c4d15663ebb0~tplv-k3u1fbpfcp-watermark.image?)

# 文生图

[文本生成图像模型各种模型详细总结](https://swarma.org/?p=37227)

[文本生成图像模型各种模型详细总结](https://zhuanlan.zhihu.com/p/593896912)
    
# CLIP



![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/720a5634affe4b089b23e73bfd8122c4~tplv-k3u1fbpfcp-watermark.image?)
## 训练
CLIP的模型结构其实非常简单：包括两个部分，即文本编码器（Text Encoder） 和图像编码器（Image Encoder) 。Text Encoder选择的是Text Transformer模型；Image Encoder选择了两种模型，一是基于CNN的ResNet（对比了不同层数的ResNet），二是基于Transformer的ViT。


![CLIP 训练过程.gif](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0f199606ba314341983aa8dc813559dc~tplv-k3u1fbpfcp-watermark.image?)

## 推理

1.  根据所迁移的数据集将所有类别转换为文本。这里以Imagenet有1000类为例，我们得到了1000个文本：`A photo of {label}`。我们将这1000个文本全部输入 Text Encoder中，得到1000个编码后的向量 ，Ti（0<=i<=1000）,这被视作文本特征。
1.  我们将需要分类的图像（单张图像）输入Image Encoder中，得到这张图像编码后的向量 I1 ，将I1与得到的1000个文本特征分别计算余弦相似度。找出1000个相似度中最大的那一个，那么评定要分类的图片与第三个文本标签（dog）最匹配，即可将其分类为狗。

## github 仓库

1. https://link.zhihu.com/?target=https%3A//github.com/openai/CLIP
1. 仓库里面有完整的 python 使用教程

tokenizer
text encoder：就是 12 层重复的 masked multi-head self-Attention + Dense + LayerNorm + ResNet

# DALL·E 2
[论文地址](https://cdn.openai.com/papers/dall-e-2.pdf)

DALL·E 2 这个模型的任务很简单：输入文本 text ，生成与文本高度对应的图片。它主要包括三个部分：CLIP，先验模块 prior 和 img decoder 。其 中CLIP 又包 含text encoder 和 img encoder 。训练和推理图如下所示，虚线上方训练 CLIP 过程；虚线下方由文本生成图像过程。

 
![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c880a8c674c9498c86b7964b649cde4f~tplv-k3u1fbpfcp-watermark.image?)



## 训练
DALL·E 2 是将其各个子模块分开训练的，最后将这些训练好的子模块拼接在一起，最后实现由文本生成图像的功能。

1. 训练 CLIP 使其能够编码文本和对应图像，这一步是与 CLIP 模型的训练方式完全一样的，目的是能够得到训练好的 text encoder 和 img encoder 。这么一来，文本和图像都可以被编码到相应的特征空间。对应上图中的虚线以上部分。

2. 训练 prior 使文本编码可以转换为图像编码，将CLIP中训练好的 text encoder 拿出来，输入文本 y ，得到文本编码 zt 。同样的，将 CLIP 中训练好的 img encoder 拿出来，输入图像 x 得到图像编码 zi 。我们希望 prior 能从 zt 获取相对应的 zi 。假设 zt  经过 prior 输出的特征为 zi′，那么我们自然希望 zi′ 与 zi 越接近越好，这样来更新我们的 prior 模块。最终训练好的 prior ，将与 CLIP 的 text encoder 串联起来，它们可以根据我们的输入文本 y 生成对应的图像编码特征 zi 了。关于具体如何训练 prior ，有兴趣的小伙伴可以精度一下原文，作者使用了主成分分析法 PCA 来提升训练的稳定性。


    ![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2eef623bc6684ca79a4a5100279a1844~tplv-k3u1fbpfcp-watermark.image?)

    在 DALL·E 2 模型中，作者团队尝试了两种先验模型：自回归式Autoregressive (AR) prior 和扩散模型 Diffusion prior  。实验效果上发现两种模型的性能相似，而因为扩散模型效率较高，因此最终选择了扩散模型作为 prior 模块。 
    
3. 训练 decoder 生成最终的图像，也就是说我们要训练decoder模块，从图像特征 zi 还原出真实的图像 x  。这个过程与自编码器类似，从中间特征层还原出输入图像，但又不完全一样。我们需要生成出的图像，只需要保持原始图像的显著特征就可以了，这样以便于多样化生成，如下图。DALL-E 2使用的是改进的 GLIDE 模型，这个模型可以根据 CLIP 图像编码的zi ，还原出具有相同与 x 有相同语义，而又不是与 x 完全一致的图像。


![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/48637af0f7aa4c338b4a6094d86f84e3~tplv-k3u1fbpfcp-watermark.image?)


## 推理
经过以上三个步骤的训练，已经可以完成 DALL·E 2 预训练模型的搭建了。我们丢掉 CLIP 中的 img encoder ，留下 CLIP 中的 text encoder ，以及新训练好的 prior 和 decoder 。这么一来流程自然很清晰了：由 text encoder 将文本进行编码，再由 prior 将文本编码转换为图像编码，最后由 decoder 进行解码生成图像。



![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/02d5cc14967044b18629b87e37a3cd87~tplv-k3u1fbpfcp-watermark.image?)
 

## 不足

- 容易将物体和属性混淆
- 对于将指定文本绘制图像中的能力不足
- 在生成复杂场景图片时，对细节处理有缺陷


# 图像分类、图生文、图像相似度、图像分割

1. 池化原理
2. 卷积原理、计算公式
3. 上采样、下采样
4. 插值
5. padding：same、valid ： https://blog.csdn.net/David_jiahuan/article/details/104722284
        
        
# NLP


1. NER、文本分类、文本生成、文生图、语义匹配、文本相似度、query 理解、召回、排序、标准化、意图识别、常见的 NLP 模型，如 w2v、RNN、LSTM、ATTENTION、TRANSFORMER、BERT、知识图谱、https://zhuanlan.zhihu.com/p/58931044
2. tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
3. tf.keras.losses.CategoricalCrossentropy
4. SparseCategoricalAccuracy
5. CategoricalAccuracy
6. BinaryCrossentropy：二分类交叉熵，只有预测类和真实类是相等时，loss 才为 0，否则 loss 就是为一个正数。而且概率相差越大 loss 就越大。这个神奇的度量概率距离的方式称为交叉熵。https://blog.csdn.net/weixin_49346755/article/details/124523232
https://blog.csdn.net/legalhighhigh/article/details/81409551

7. Adam、adamw
8. 随机梯度下降 sgd
9. 激活函数：激活函数给神经元引入了非线性的因素，使得神经网络可以逼近任何非线性函数，可以得到学习和表示几乎任何东西的神经网络模型。常见的有 relu、sigmoid（二分类）、tahn，以及求导。https://brickexperts.github.io/2019/09/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/ 
10. softmax
11. sigmoid
12. GroupNorm、LayerNorm、BatchNorm
13. 特征、数据归一化 normalization:适合有不同尺度和范围的特征或数据中，原因是特征要乘以模型权重，输出值和梯度的规模受到输入规模的影响，这样可能加速训练，并且更加稳定
14. 权重正则化：Dropout、层正则化、批次正则化、L1、L2：通过对模型参数（权重）的额外约束来减少过拟合的风险。这种约束通常通过在模型的损失函数中添加正则化项实现
15. L1：在损失函数中添加权重的绝对值之和乘以一个正的超参数。L1 正则化推动模型参数向零稀疏，促使一些权重变为精确的零，也就是鼓励稀疏模型。
16. L2(权重衰减)：在损失函数中添加权重的平方和乘以一个正的超参数。L2 正则化推动模型参数向零，但相对于 L1 正则化，它不会将权重精确地变为零。
17. Dropout：正则化技术、训练时期使得部分神经元失活，既可以对输入数据的 Dropout ，又能对隐层的 Dropout

- 为何能解决过拟合？通过 Bagging 方式随机抽取不同的训练数据，训练不同的决策树以构成随机森林，采用投票方式完成决策任务。 而 Dropout 可以被视为一种极端形式的 Bagging ，设置丢弃神经元的概率之后，每次训练数据的时候都相当于是在训练一个新的模型，
- 训练阶段为了保证输入和保持不变，将没有置为 0 的结果值都除了保留概率，保证了输入的和不变，也保证了输出的期望 x 不变

18. tf-idf
19. ngram
20. cache() ：在整个数据集上进行缓存，以便在多次迭代中重复使用数据，从而提高性能。
21. prefetch :在迭代数据时，可以在 CPU 处理当前批次的数据的同时，异步预加载下一批次的数据到 GPU 上，提高数据加载和模型训练的并行度。
22. Multi-GPU distributed training：https://keras.io/guides/distributed_training_with_tensorflow/
23. 编码：one-hoe、数字化、向量化、Warm-start embedding
24. NNLM ：2003年第一次用神经网络来解决语言模型，能够得到语言序列的概率分布，当时语言模型的问题主要是纬度灾难、向量稀疏、计算量巨大，使用 NNLM 能解决这些问题，而且模型能提升语言的泛化性，并且能得到副产品词向量。

    公式 y = b + Wx + Utanh(d+Hx) 共三层输入层、隐藏层、输出层，另外输入层还有个 word embedding 矩阵需要训练，隐藏层的输入需要将所有 lookup 出来的词向量进行拼接

    缺点就是模型结构简单、计算复杂(尤其输出层)，softmax 低效
    https://blog.51cto.com/Lolitann/5333935


25. GloVe：GloVe的思想和word2vec的思想接近，但是GloVe是从全局的角度出发，构建一个基于上下文窗口的共现计数矩阵，每一次的训练都是基于整个语料库进行分析的，所以它才叫 Global Vector，而 word2vec 则是从一个个句子出发进行训练的

    https://blog.csdn.net/jesseyule/article/details/99976041

26. huber：当δ~ 0时，Huber损失会趋向于MAE；当δ~ ∞（很大的数字），Huber损失会趋向于MSE。


    ![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f4abd66e3b774eaeafb9b9d96f482298~tplv-k3u1fbpfcp-watermark.image?)
27. MAE(mean absolute error)，即平均绝对值误差


    ![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/57cf58a0208d4912b5a723b88014b34c~tplv-k3u1fbpfcp-watermark.image?)

28. MSE(mean squared error)，即均方误差


    ![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6d10f3cfd35f4d6bbebc8c721e691ca5~tplv-k3u1fbpfcp-watermark.image?)

29. dsf
30.  

31. 预训练模型
32. 迁移学习、微调、局部微调、整体微调
33. 过拟合：过拟合是指模型在训练数据上表现得比在测试数据上更好，但当面对未见过的数据时，性能却下降的现象，原因有：训练数据太少、特征冗余、数据噪声太多、数据分布不均匀、模型太复杂、训练时间太长。解决办法：充足有效的训练数据、降低模型复杂度、数据归一化、权重正则化（L1、L2、Dropout）、数据增强。
34. 欠拟合：欠拟合是指模型在训练和测试数据上的性能都较差，未能很好地捕捉数据的模式，原因有：数据特征不足、数据量太小、模型复杂度低、训练时间短、超参数选择不当（学习率）
35. 超参数选择或者超参数调整，主要包括模型和算法超参数，Keras Tuner 中有四种工具可选：`RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`，先使用训练数据找出最优超参数，然后使用训练数据用找出来的最优超参数去重新训练模型
36. 前向传播
37. 反向传播
38. 损失函数：它衡量模型的预测与标签的偏离程度，损失越小越好
39. 优化器：优化器将计算出的梯度应用于模型参数以最小化损失函数，损失越低，模型的预测就越好。
40. RNN：权重计算 https://juejin.cn/post/6972340784720773151

    -   输入门权重矩阵 `W_i`：`units * (input_dim + units)`
    -   输入门偏置向量 `b_i`：`units`
    -   输出权重矩阵 `W_o`：`units * units`
    -   输出偏置向量 `b_o`：`units`
    -   Total 参数数量为上述各项之和。

37. LSTM：权重参数计算 (units * (input_dim + units)  + units ) * 4 https://juejin.cn/post/6973082167970627620
    -   输入门权重矩阵 `W_i`：`units * (input_dim + units)`
    -   输入门偏置向量 `b_i`：`units`
    -   遗忘门权重矩阵 `W_f`：`units * (input_dim + units)`
    -   遗忘门偏置向量 `b_f`：`units`
    -   细胞状态权重矩阵 `W_c`：`units * (input_dim + units)`
    -   细胞状态偏置向量 `b_c`：`units`
    -   输出门权重矩阵 `W_o`：`units * (input_dim + units)`
    -   输出门偏置向量 `b_o`：`units`
    -   Total 参数数量为上述各项之和。

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bf9c8e82555241bcb9a1b3de59a49649~tplv-k3u1fbpfcp-watermark.image?)
![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9421f89ae19a4914aa8372caaca254f2~tplv-k3u1fbpfcp-watermark.image?)

38. **GRU 单元：** reset_after=False 也就是 SimpleRNN， 3 * (units * (input_dim + units) + units)

    -   重置门权重矩阵 `W_r`：`units * (input_dim + units)`
    -   重置门偏置向量 `b_r`：`units`
    -   更新门权重矩阵 `W_z`：`units * (input_dim + units)`
    -   更新门偏置向量 `b_z`：`units`
    -   新状态权重矩阵 `W_h`：`units * (input_dim + units)`
    -   新状态偏置向量 `b_h`：`units`

    CuDNNGRU 為了加速計算因此計算方式與一般 RNN 有稍微不同 tf2 默认 reset_after=True ，计算每个门的时候多一组偏置项。  3 * (units * (input_dim + units) + units + units)

    -   重置门权重矩阵 `W_r`：`units * (input_dim + units)`
    -   重置门偏置向量 `b_r`：`units + units`
    -   更新门权重矩阵 `W_z`：`units * (input_dim + units)`
    -   更新门偏置向量 `b_z`：`units + units`
    -   新状态权重矩阵 `W_h`：`units * (input_dim + units)`
    -   新状态偏置向量 `b_h`：`units + units`

39. BERT 输出有 word_id（包含了 \[CLS\]:101 ，\[SEP\]: 10），mask，type_id ，可以输出 pool value， encoded value（每一层Block 的输出，包括最后一层的输出【B，S，H】）

    建议从参数较少的小型 BERT 开始使用，因为它们的微调速度更快。 如果您喜欢小型模型但精度更高，可以用 ALBERT 。 如果您想要更高的准确性，可以用经典的 BERT 之一如 Electra、Talking Heads 或 BERT Expert。除此之外还有规模更大精度更高的版本，但是无法在单个 GPU 上进行微调，需要使用 TPU 。

40. 文本生成：训练数据制作、模型选择、选则token三种方式（选最大，按照概率分布抽样、温度控制temperature选择1则退化为第一种方式）
41. seq2seq 优化技巧

    - 使用更加复杂度的 RNN 变体
    - 双向的特征拼接
    - 更优化的预训练 Embedding
    - 使用 Attention
    - 如果是翻译类任务，可以使用不同语种翻译训练过程，提升某一语种的翻译性能
42. seq2seq 相似度计算，加入 attention ，时间复杂度 `O(m+m\*t)`，没有 attentino 时间复杂度为 `O(m+t)` 。
    - decoder 时候使用两个参数矩阵对状态 si 与每个时刻的状态输出 hi 进行非线性变化，然后进行 softmax
    - decoder 时候使用算出 hi 和 si 的非线性变化 ki 和 qi ，然后两者求内积，最后进行 softmax

43. Seq2Seq 做机器翻译，当输入句子的单词在 20 个附近时的效果最佳，当输入句子的单词超过 20 个的时候，效果会持续下降，这是因为 Encoder 会遗忘某些信息。而在 Seq2Seq 的基础上加入了 Attention 机制之后，在输入单词超过 20 个的时候，效果也不会下降。
44. self-attention 
45. BERT ：Bidirectional Encoder Representations from Transformers ，其实 BERT 的目的就是预训练 [Transformers](https://link.juejin.cn/?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1706.03762.pdf "https://arxiv.org/pdf/1706.03762.pdf") 模型的 Encoder 网络。三个基本任务分别是：对遮盖的 15% token 进行预测 ，判断两句话是不是相邻的（包括正负样本），第三个任务是混合起来前两个

    110M 参数-340M参数 ，主要区别在 layer, hidden, heads

46. Generative Pre-trained Transformer ，

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2ba10af6316d41369aeff17edb19b9cd~tplv-k3u1fbpfcp-watermark.image?)
47. GPT 2018年，参数量 1.17 亿 ，训练数据 5GB，。 由 12 层简化的 Transformer 的 Decoder Block 组成，每个 Block 中有 Masked Multi self-Attention 和 Dense ，序列长度 512 。GPT-1的训练分为无监督的预训练和有监督的模型微调，而需要注意的另外一个细节，是fine-tune的loss函数，既包含了下游任务的loss，也包含了语言模型的loss（预测下一个单词），这么做的目的是在做垂直领域任务的时候，保持着自己本身的这种语言模型的性质，不要把语言本身给忘掉。

主要解决得是监督任务得两个主要弊端：需要大量标签数据，模型领域化无法真正泛化去理解语言。通过对输入和输出的不同处理方式，在很多任务上 SOTA ，可以完成文本分类、相似度计算、问答、推理等任务。



48. GPT-2，2019年，结构上无太大变化，只是使用了更多的网络参数和更大的数据集，在 40 G 的 WebText 数据集上进行训练，目标是使用无监督的预训练模型做有监督的任务，作者认为当一个语言模型的容量足够大时，它就足以覆盖所有的有监督任务，即训练完翻译的任务后，也就是学会了该数据范围的问答。参数量最低 117M 参数，最大 1542M 参数量，约 15 亿，主要区别是Block 的 layer 和 hidden ，序列长度扩大到 1024。在众多任务中获得 SOTA ，初步具备了zero-shot 和 few-shot能力。

GPT-2 进行模型调整的主要原因在于，随着模型层数不断增加，梯度消失和梯度爆炸的风险越来越大。 GPT-2的最大贡献是验证了通过海量数据和大量参数训练出来的 GPT 模型有迁移到其它类别任务中而不需要额外的训练的能力。


49. GPT-3， 2020年，参数量 1750 亿，数据 45 TB 过滤出 570GB ，模型结构没有变化，只是注意力头增多具备 zero-shot 或者 few-shot ，在下游任务中表现出色，从理论上讲GPT-3也是支持fine-tuning的，但是fine-tuning需要利用海量的标注数据进行训练才能获得比较好的效果，但是这样也会造成对其它未训练过的任务上表现差，所以GPT-3并没有尝试fine-tuning。

    GPT-3的本质还是通过海量的参数学习海量的数据，然后依赖transformer强大的拟合能力使得模型能够收敛。基于这个原因，GPT-3学到的模型分布也很难摆脱这个数据集的分布情况。缺点有：
    输出错误或者不合逻辑，对于 PROMPT 无法正确理解
    可能包含一些非常敏感的内容，例如种族歧视，性别歧视，宗教偏见等；
    受限于transformer的建模能力，GPT-3并不能保证生成的一篇长文章的连贯性，存在下文不停重复上文的问题。
    数据量和参数量的骤增并没有带来智能的体感
50. GPT3.5（InstructGPT），GPT-3纵然很强大，但是对于人类的指令理解的不是很好，3H 指出了训练 GPT3.5目标是Helpful、Honest、Harmless，原文分为了三个步骤进行训练：有监督微调SFT，奖励模型训练RM，强化学习训练RLHF；实际上可以把它拆分成两种技术方案，一个是有监督微调（SFT），一个是基于人类反馈的强化学习（RLHF）

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8bedd44e7b774ef8a31a1c5705bbe1ba~tplv-k3u1fbpfcp-watermark.image?)


SFT:人类标注的<prompt,answer>数据来Fine-tune GPT-3模型，以使其初步具备理解人类prompt中所包含意图，并根据这个意图给出相对高质量回答的能力。这一步骤中包含了1.2万条训练数据。

RM：RM结构是将SFT训练后的模型的最后的嵌入层去掉后的模型，是一个回归模型。它的输入是prompt和Reponse，输出是奖励值。通过人工标注数据来训练回报模型。 随机抽样prompt，并使用第一阶段Fine-tune好的冷启动模型，生成K个不同的回答。然后，标注人员根据相关性、信息性和有害信息等标准，对K个结果进行排序，生成排序结果数据。接下来，研究者使用这个排序结果数据进行pair-wise learning to rank训练模式，训练回报模型。RM模型接受一个输入<prompt,answer>，给出评价回答质量高低的回报分数Score。对于一对训练数据<answer1,answer2>，假设人工排序中answer1排在answer2前面，那么Loss函数则鼓励RM模型对<prompt,answer1>的打分要比<prompt,answer2>的打分要高。

RLHF：随机抽取 PROMPT，让模型来生成回答。用RM模型对回答进行评估，并给出一个分数作为回报。训练LLM模型生成的答案能够获得高分数，根据得到的回报分数来使用 PPO 算法更新模型的参数，以便让模型生成更好的回答。

PPO 的核心思想之一是通过对比新策略和旧策略之间的差异来更新策略。训练策略网络时，通过最大化优势函数，即新策略相对于旧策略的增益来更新网络参数。但在更新时，为了保持策略的变化不会过大通常会引入一个 Kullback-Leibler（KL）散度的约束，也就是让LM的输出在RM的评估体系中拿到高分，同时不能失去LM输出通顺文本能力，即优化后的模型输出和原模型输出的KL散度越接近越好。而chatgpt 中的策略函数就是预训练GPT得到的生成模型 ， 价值函数则是后续通过人工标注数据训练的打分模型。当然chatgpt对PPO损失做了一定的修改，增加了语言生成模型的损失，希望生成模型生成的句子越通顺越好。


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4041553fb2854026a0710990d5b30587~tplv-k3u1fbpfcp-watermark.image?)

51. 指示学习（Instruct Learning）和提示（Prompt Learning）学习

    指示学习和提示学习的目的都是去挖掘语言模型本身具备的知识。指示学习的优点是它经过多任务的微调后，也能够在其他任务上做 zero-shot ，而提示学习都是针对一个任务的,泛化能力不如指示学习:

    提示学习：激发语言模型的**补全能力**,如：给女朋友买了这个项链，她很喜欢，这个项链太XX了。
    指示学习：激发语言模型的**理解能力**,如：这句话的情感是非常正向的：给女朋友买了这个项链，她很喜欢。

52. Transformers:Transformer 是 Seq2Seq 模型，包括了一个 Encoder 和一个 Decoder 。Transformer 不是 RNN 模型，它没有循环结构。Transformer 完全基于 Attention 和 Self-Attention 搭建而成。

    - Transformer 的 Encoder 网络就是靠 6 个 Block 搭建而成的，每个 Block 中有 Multi-Head Self-Attention Layer 和 Dense Layer ，输入序列大小是【d，m】，输出序列大小是【d，m】。
    - Transformer 的 Decoder 网络也是靠 6 个 Block 搭建而成的，每个 Block 中有 Masked Multi-Head Self-Attention Layer、 Multi-Head Attention Layer 和 Dense Layer ，输入序列有两个，一个序列大小是【d，m】，另一个序列大小为【d，t】，输出序列大小是【d，t】。

53. 维特比算法
54. XLA（Accelerated Linear Algebra）是一种针对线性代数计算的加速器，旨在优化高性能计算和深度学习模型的执行速度。XLA 可以优化计算图，使其更适应硬件加速器，提高深度学习模型的训练。XLA（Accelerated Linear Algebra）默认是启用的，而且无需额外的配置。TensorFlow 2.x 在默认情况下会自动使用 XLA 来优化计算图，以提高深度学习模型的执行效率。
55. PolynomialDecay

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/67cef1ac82a4481eaa3e2edbe41a2f12~tplv-k3u1fbpfcp-watermark.image?)

56. 迁移学习（Transfer Learning）、微调（fine-tuning）、one-shot、few-shot

    **迁移学习（Transfer Learning）** 通过在一个任务上学到的知识来改善在另一个相关任务上的性能。源领域和目标领域之间的任务通常是相关但不同的。如翻译，例如学会了骑自行车的人也能较快的学会骑电动车。较为常用的一种迁移学习方式是利用预训练模型进行微调。为了解决下面问题：

    -   一些研究领域只有少量标注数据，且数据标注成本较高，不足以训练一个足够鲁棒的神经网络。
    -   大规模神经网络的训练依赖于大量的计算资源，这对于一般用户而言难以实现。
    -   应对于普适化需求的模型，在特定应用上表现不尽如人意。

    **微调（Fine-tuning）** 是迁移学习的一种具体实践方式。在微调中，我们首先在源领域上训练一个模型，然后将这个模型应用到目标领域上，但不是从头开始训练。相反，我们对模型的一部分（或全部）进行调整以适应目标领域的任务。通常，这包括冻结模型的一些层，然后对未冻结的层进行微小的调整。

    **联系：**

    -   微调是迁移学习的一种策略，迁移学习并不仅限于微调,其他迁移学习的策略包括特征提取、共享表示学习等。微调是其中最常见的一种方法之一，特别在深度学习中，通过在预训练模型的基础上进行微调，可以更好地适应目标任务。
    -   在迁移学习中，微调通常是指在源任务上训练的模型参数的调整，以适应目标任务。
    -   微调涉及到从一个任务到另一个任务的知识传递，这与迁移学习的目标一致。

    **One-shot Learning：**: 在一次学习中，模型只能从一个样本（一张图片、一个数据点等）中学到目标任务的知识。 通常应用于样本非常有限、甚至只有一个样本的情况，要求模型能够在仅有极少信息的情况下完成任务。

    **Few-shot Learning：** ： 在少次学习中，模型从几个（通常是很少的）样本中学到目标任务的知识。Few-shot 可能包含从两到几十个样本的情况。 使用于数据较为稀缺的情况，但相对于 One-shot Learning 来说，Few-shot Learning 允许更多的样本用于训练，提供了一定的上下文和信息。

    **zero-shot** 则是不提供示例，只是在测试时提供任务相关的具体描述。实验结果表明，三个学习方式的效果都会随着模型容量的上升而上升，且few shot > one shot > zero show。


![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/96567c4abd154c34989ffc3a49542453~tplv-k3u1fbpfcp-watermark.image?)
57. 语言模型是这样一个模型：**对于任意的词序列，它能够计算出这个序列是一句话的概率** https://zhuanlan.zhihu.com/p/32292060
58. 典型的大规模数据处理流程


![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e113d817a8534268a89fb14d6fa4c9cb~tplv-k3u1fbpfcp-watermark.image?)
59. sampling methods  ： https://zhuanlan.zhihu.com/p/453286395
    - GreedySampler
    - BeamSampler
    - Temperature Sampling
    - RandomSampler
    - TopKSampler
    - TopPSampler
    
 

  
 
        
        
        
# Elasticsearch 
1. 倒排索引
2. BM25
3. 架构
4. 优化技巧
5. 



# python
1. 基本语法
2. flask + gunicorn


# 神经网络压缩与加速:

1. 减少其存储和计算成本变得至关重要
2. 不是所有神经元都起作用，仅仅只有少部分（5-10%）权值参与着主要的计算，仅仅训练小部分的权值参数就可以达到和原来网络相近的性能
3. 在移动设备上运行


    ![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1a3087cd2ade4de2a7977266629cb703~tplv-k3u1fbpfcp-watermark.image?)

# 机器学习十大常见算法（唐宇迪）


    

# CPU/GPU推理加速

1. 量化
2. 剪枝
3. ONNX-CPU
4. ONNX-GPU


# Tensorflow 框架
1. 分布式训练
2. 剪枝
3. 量化
4. 保存模型，只保存权重，保存模型和权重，模型可以保存为 .keras 、SavedModel、.h5，如果需要在 TensorFlow 生态系统中进行跨平台部署，则推荐使用 SavedModel 格式；如果只是保存模型结构和权重，HDF5 可以是一个不错的选择。 Keras 格式则提供了在 TensorFlow 中使用 Keras API 时的一种保存和加载方式。
5.  NumPy 数组和 tf.Tensors 之间最明显的区别是：一是张量可以由加速器内存（如 GPU、TPU）支持。二是张量是不可变的。

 





 









# 其他
- reading book
- `leetcode 经典 200 题`
- 掘金、CSDN
- stable diffusion 
- 健身
- 公务员
- options
- 论文，架构，人才，年度优秀员工
- 副业 https://github.com/wangdayaya/aimoneyhunter