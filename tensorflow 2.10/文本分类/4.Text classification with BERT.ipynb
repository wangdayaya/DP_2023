{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f5cc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98bb4aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", 'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.']\n",
      "[0, 0]\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "batch_size = 32\n",
    "train_ds, test_ds = tfds.load(\"imdb_reviews\", split=['train', 'test'], as_supervised=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"D:/bert-base-uncased\")\n",
    "texts = []\n",
    "labels = []\n",
    "for t,l in train_ds:\n",
    "    texts.append(t.numpy().decode('utf-8'))\n",
    "    labels.append(l.numpy())\n",
    "for t,l in test_ds:\n",
    "    texts.append(t.numpy().decode('utf-8'))\n",
    "    labels.append(l.numpy())\n",
    "print(texts[:2])\n",
    "print(labels[:2])\n",
    "print(len(texts))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eba5a65b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape (50000, 128)\n",
      "attention_mask.shape (50000, 128)\n",
      "labels.shape (50000,)\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.batch_encode_plus(texts, \n",
    "                                 add_special_tokens=True,\n",
    "                                 max_length=max_length,\n",
    "                                 padding='max_length',\n",
    "                                 truncation=True,\n",
    "                                 return_attention_mask=True,\n",
    "                                 return_tensors='tf')\n",
    "input_ids = encoded['input_ids']\n",
    "attention_mask = encoded['attention_mask']\n",
    "labels = tf.convert_to_tensor(labels)\n",
    "print(\"input_ids.shape\", input_ids.shape)\n",
    "print(\"attention_mask.shape\",attention_mask.shape)\n",
    "print(\"labels.shape\",labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((input_ids, attention_mask), labels)).shuffle(len(texts)).batch(batch_size)\n",
    "data_size = len(dataset)\n",
    "train_size = int(0.8*data_size)\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30eb7caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  101  2298  1010  1045  1005  2310  8134  2439  2035  3246  1999 20814\n",
      "   2044  3666  2037 14751  1000  2718  1010  1000  1996  6248  3428  2316\n",
      "   2265  1010  1998  1000 24582  2906  2135  1000  2003  2053  6453   999\n",
      "   2065  2017  4033  1005  1056  4384  1010 24582  2906  2135  2003  2085\n",
      "   1996  1001  1015  2718  1056 28394  2078 13130  2006  2547  2157  2085\n",
      "    999  2044  4994  2023  1010  1045  2787  2000  3422  1037  2261  4178\n",
      "   2870  2000  2156  2054  1996  1044 18863  2001  2055   999  1045  2031\n",
      "   2028  2773  2000  6235  2023  2265  1999  2236  1012  1012  1012  1000\n",
      "   3947  3238   999   999   999  1000  1045  3685  2903  2008  4907 15159\n",
      "   2052  2175  2023  2659  1998  2191  2242  2023 10231  7685   999   999\n",
      "    999  2009  1005  1055  9202   999   999   102]\n",
      " [  101  1045  3427  2023  6823  1010  3202  2128 12155  8630  2009  1010\n",
      "   3427  2009  2153  1998  4191  3807  2004  2524  1012  1045  6118 16755\n",
      "   2023  6823  2005  2216  2040  2024  2025  5223  3993  1997  1010  2021\n",
      "   8796  2105  9099  6961 23096  2015  1012  2009  3065  2017  2008  9099\n",
      "   6961  3775 17456  2003  1037  3444  1010  2738  2084  1996 15700  1997\n",
      "   2028  1005  1055  2108  1012  1996  4038  2003  2025  2309  3277  1012\n",
      "   2023  2158  2003  8235  1012  2035  5888  2323  2004 20781  2000  2010\n",
      "   2504  1997  2064  7983  1010  4454  1998  5848  1012   102     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]], shape=(2, 128), dtype=int32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(2, 128), dtype=int32)\n",
      "\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for (a,b),c in train_dataset.take(1):\n",
    "    print(a[:2])\n",
    "    print()\n",
    "    print(b[:2])\n",
    "    print()\n",
    "    print(c[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "577b7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at D:/bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at D:/bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertModel.from_pretrained(\"D:/bert-base-uncased\")\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = True\n",
    "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_mask)\n",
    "cls_token = bert_output.last_hidden_state[:,0,:]\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)\n",
    "model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f8baa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 129s 98ms/step - loss: 0.3081 - binary_accuracy: 0.8677 - val_loss: 0.1742 - val_binary_accuracy: 0.9350\n",
      "Epoch 2/3\n",
      "1250/1250 [==============================] - 121s 97ms/step - loss: 0.1980 - binary_accuracy: 0.9222 - val_loss: 0.1325 - val_binary_accuracy: 0.9543\n",
      "Epoch 3/3\n",
      "1250/1250 [==============================] - 121s 97ms/step - loss: 0.1208 - binary_accuracy: 0.9561 - val_loss: 0.0465 - val_binary_accuracy: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e4b0708a00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "model.fit(train_dataset, epochs=3, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08641a4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_6\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=string>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis was an absolutely terrible movie.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filek2colj3l.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_6\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=string>]\n"
     ]
    }
   ],
   "source": [
    "model.predict([\"This was an absolutely terrible movie.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975dffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2.10.0-py-3.10",
   "language": "python",
   "name": "tf-gpu-2.10.0-py-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
