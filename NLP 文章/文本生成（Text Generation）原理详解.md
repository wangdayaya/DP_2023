## 承上启下

上一篇文章我们介绍了 RNN 相关的基础知识，现在我们介绍文本生成的基本原理，主要是为了能够灵活运用 RNN 的相关知识，真实的文本生成项目在实操方面比这个要复杂，但是基本的原理是不变的，这里就是抛砖引玉了。RNN 基础知识回顾链接：https://juejin.cn/post/6972340784720773151

## 原理

我们这里用到了 RNN 来进行文本生成，其他的可以对时序数据进行建模的模型都可以拿来使用，如 LSTM 等。这里假如已经训练好一个 RNN 模型来预测下一个字符，假如我们限定了输入的长度为为 21 ，这里举例说明：

	input：“the cat sat on the ma”
	
把 21 个字符的文本分割成字符级别的输入，输进到模型中，RNN 来积累输入的信息，最终输出的状态向量 h ，然后经过全连接层转换和 Softmax 分类器的分类，最终输出是一个候选字符的概率分布。

在上面的例子中，输入“the cat sat on the ma”，最后会输出 26 个英文字母和其他若干用到的字符（如可能还有标点，空格等）的概率分布。

	"a" --> 0.05
	"b" --> 0.03
	"c" --> 0.054
	...
	"t" --> 0.06
	...
	"，" --> 0.01
	"。" --> 0.04

此时预测的下一个字符“t”概率值最大，所以选择“t”作为下一个字符，我们之后将“t”拼接到“the cat sat on the ma”之后得到“the cat sat on the mat”，然后我们取后 21 个字符“he cat sat on the mat”，输入到模型中

	input：“he cat sat on the mat”
	
此时加入预测下一个字符的概率分布中“。”的概率最大，我们就取“。”拼接到“the cat sat on the mat”之后，得到“the cat sat on the mat。”，如果还需要继续进行下去，则不断重复上面的过程。如果我们的文本生成要求到此结束，则最终得到了文本

	the cat sat on the mat。
	
通常我们要用和目标相同的数据进行训练。如想生成诗词，就用唐诗宋词去训练模型，像生成歌词，就用周杰伦的歌词去训练。

## 选取预测的下一个字符的三种方式
一般在得到概率分布，然后去预测下一个字符的时候，会有三种方法。
	


第一种方法就是像上面提到的，选择概率分布中概率最大的字符即可。这种方法虽然最简单，但是效果并不是最好的，因为几乎预测字符都是确定的，但是不能达到多元化的有意思的字符结果。公式如下：

	next_index = np.argmax(pred)

第二种方法会从多项分布中随机抽样，预测成某个字符的概率为多少，则它被选取当作下一个字符的概率就是多少。在实际情况中往往概率分布中的值都很小，而且很多候选项的概率相差不大，这样大家被选择的概率都差不多，下一个字符的预测随机性就很强。假如我们得到预测成某个正确字符的概率为 0.1 ，而预测成其他几个字符的概率也就只是稍微低于 0.1 ，那么这几个字符被选取当作下一个字符的概率都很相近。这种方式过于随机，生成的文本的语法和拼写错误往往很多。公式如下：

	next_onehot = np.random.multimomial(1, pred, 1)
	next_index = np.argmax(next_onehot)

第三种方法是介于前两种方法之间的一种，生成的下一个字符具有一定的随机性，但是随机性并不大，这要靠 temperature 参数进行调节, temperature 是在 0 到 1 之间的小数，如果为 1 则和第一种方法相同，如果为其他值则可以将概率进行不同程度的放大，这表示概率大的字符越大概率被选取到，概率小的字符越小概率被选择到，这样就可以有明显的概率区分度，这样就不会出现第二种方法中的情况。公式如下所示：

	pred = pred ** (1/temperature)
	pred = pred / np.sum(pred)

	
## 训练

假如我们有一句话作为训练数据，如下：

	Machine learning is a subset of artificial intelligence.
	
我们设置两个参数 len = 5 和 stride = 3 ，len 是输入长度，stride 是步长，我们将输入 5 个字符作为输入，然后输入下一个字符作为标签，如下

	input：“Machi”
	target：“n”

然后因为我们设置了 stride 为 3 ，所以我们在文本中向右平移 3 位，然后又选择 5 个字符作为输入，之后的一个字符作为标签，如下：

	input：“hine ”
	target：“l”

如此往复，不断向右平移 3 个字符，将新得到的 5 个字符和接下来的 1 个字符作为标签作为训练数据输入到模型中，让模型学习文本内部的特征。其实训练数据就是（字符串，下一个字符）的键值对。此时得到的所有训练数据为：

	input：'Machi'
	target：'n'
	
	input：'hine '
	target：'l'
	
	input：'e lea'
	target：'r'
	
	...	
	input：'ligen'
	target：'c'

然后用这些训练数据进行大量的训练得到的模型，就可以用来生成新的文本啦！。
	
## 总结

训练模型的流程大致需要三个过程：

	1.将训练数据整理成（segment，next_char）的组合
	2.用 one-hot 将字符编码，segment 编码成 l*v 的向量，next_char 编码成 v*1 的向量，l 是输入长度，v 是字符总个数
	3.构建一个网络，输入是 l*v 的矩阵，然后通过 RNN 或者 LSTM 捕捉文本特征，然后将最后的特征进行全连接层进行转换，全连接层用 Softmax 作为激活函数，最后输出一个 v*1 的概率分布，下一个字符的选择方式可以看上面的内容。
	
生成文本的流程大致需要三个过程：

	一般在已经训练好模型的情况下，我们要输入字符串当作种子输入，让其作为我们接下来要生成文本的开头，然后不断重复下面的过程：
		a）把输入使用 one-hot 向量表示，然后输入到模型中
		b）在神经网络输出的概率分布中选取一个字符，作为预测的下一个字符
		c）将预测的字符拼接到之前的文本后，选取新的输入文本


## 案例
这里有我之前实现的两个小案例，可以用来复习 RNN 和 LSTM 的相关知识，觉好留赞。

	https://juejin.cn/post/6949412997903155230
	https://juejin.cn/post/6949412624215834638
	
另外 github 上也有很多开源的文本生成项目，项目实现要稍微复杂一点，但是原理和我介绍的一样，我这里介绍两个。

	https://github.com/wandouduoduo/SunRnn
	https://github.com/stardut/Text-Generate-RNN
	


