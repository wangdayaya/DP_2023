{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd016327-6709-403a-9aed-4e0bc7ec901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "D:\\anaconda\\envs\\tf-gpu-2.10.0-py-3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from tqdm import tqdm\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "for gpu in  tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388914ec-bd71-4b36-90a3-88aa075fb081",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 384\n",
    "configuration = BertConfig()\n",
    "tokenizer = BertWordPieceTokenizer(\"D:/bert-base-uncased/vocab.txt\", lowercase=True)\n",
    "\n",
    "train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
    "train_path = tf.keras.utils.get_file(\"train.json\", train_data_url)\n",
    "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "eval_path = tf.keras.utils.get_file(\"eval.json\", eval_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe5d8db-bd63-4395-a391-5fa91599c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample:\n",
    "    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "\n",
    "    def preprocess(self):\n",
    "        context = self.context\n",
    "        question = self.question\n",
    "        answer_text = self.answer_text\n",
    "        start_char_idx = self.start_char_idx\n",
    "        context = \" \".join(str(context).split())\n",
    "        question = \" \".join(str(question).split())\n",
    "        answer = \" \".join(str(answer_text).split())\n",
    "\n",
    "        # 如果结束位置超出上下文长度就直接跳过返回\n",
    "        end_char_idx = start_char_idx + len(answer)\n",
    "        if end_char_idx >= len(context):\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        # 上下文长度的全 0 列表中将答案位置都标记为 1\n",
    "        is_char_in_ans = [0] * len(context)\n",
    "        for idx in range(start_char_idx, end_char_idx):\n",
    "            is_char_in_ans[idx] = 1\n",
    "\n",
    "        # 找到答案对应的子token的所有位置\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "        ans_token_idx = []\n",
    "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "            if sum(is_char_in_ans[start:end]) > 0:\n",
    "                ans_token_idx.append(idx)\n",
    "        if len(ans_token_idx) == 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        start_token_idx = ans_token_idx[0]\n",
    "        end_token_idx = ans_token_idx[-1]\n",
    "\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        padding_length = max_len - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + [0] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "            token_type_ids = token_type_ids + [0] * padding_length\n",
    "        elif padding_length < 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.start_token_idx = start_token_idx\n",
    "        self.end_token_idx = end_token_idx\n",
    "        self.context_token_to_char = tokenized_context.offsets\n",
    "\n",
    "with open(train_path) as f:\n",
    "    raw_train_data = json.load(f)\n",
    "\n",
    "with open(eval_path) as f:\n",
    "    raw_eval_data = json.load(f)\n",
    "\n",
    "\n",
    "def create_squad_examples(raw_data):\n",
    "    squad_examples = []\n",
    "    for item in tqdm(raw_data[\"data\"]):\n",
    "        for para in item[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
    "                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
    "                squad_eg = SquadExample(question, context, start_char_idx, answer_text, all_answers)\n",
    "                squad_eg.preprocess()\n",
    "                squad_examples.append(squad_eg)\n",
    "    return squad_examples\n",
    "\n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "        if item.skip == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "    x = [ dataset_dict[\"input_ids\"], dataset_dict[\"token_type_ids\"],  dataset_dict[\"attention_mask\"],]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2230032-79fe-43b9-896d-3f73a70bbbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 442/442 [00:24<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599 条训练样本\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:03<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570 条训练样本\n"
     ]
    }
   ],
   "source": [
    "train_squad_examples = create_squad_examples(raw_train_data)\n",
    "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
    "print(f\"{len(train_squad_examples)} 条训练样本\")\n",
    "\n",
    "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} 条训练样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f928bd93-2d55-4287-bf9c-64a4db30e1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'world', '!', '[SEP]']\n",
      "[101, 7592, 1010, 2088, 999, 102]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[(0, 0), (0, 5), (5, 6), (7, 12), (12, 13), (0, 0)]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "[1, 0, 0, 0, 0, 1]\n",
      "[]\n",
      "[[  101  6549  2135  1010  1996  2082  2038  1037  3234  2839  1012 10234\n",
      "   1996  2364  2311  1005  1055  2751  8514  2003  1037  3585  6231  1997\n",
      "   1996  6261  2984  1012  3202  1999  2392  1997  1996  2364  2311  1998\n",
      "   5307  2009  1010  2003  1037  6967  6231  1997  4828  2007  2608  2039\n",
      "  14995  6924  2007  1996  5722  1000  2310  3490  2618  4748  2033 18168\n",
      "   5267  1000  1012  2279  2000  1996  2364  2311  2003  1996 13546  1997\n",
      "   1996  6730  2540  1012  3202  2369  1996 13546  2003  1996 24665 23052\n",
      "   1010  1037 14042  2173  1997  7083  1998  9185  1012  2009  2003  1037\n",
      "  15059  1997  1996 24665 23052  2012 10223 26371  1010  2605  2073  1996\n",
      "   6261  2984 22353  2135  2596  2000  3002 16595  9648  4674  2061 12083\n",
      "   9711  2271  1999  8517  1012  2012  1996  2203  1997  1996  2364  3298\n",
      "   1006  1998  1999  1037  3622  2240  2008  8539  2083  1017 11342  1998\n",
      "   1996  2751  8514  1007  1010  2003  1037  3722  1010  2715  2962  6231\n",
      "   1997  2984  1012   102  2000  3183  2106  1996  6261  2984  9382  3711\n",
      "   1999  8517  1999 10223 26371  2605  1029   102     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]] \n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] \n",
      " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[114] \n",
      " [121]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello, world!\").tokens)\n",
    "print(tokenizer.encode(\"Hello, world!\").ids)\n",
    "print(tokenizer.encode(\"Hello, world!\").type_ids)\n",
    "print(tokenizer.encode(\"Hello, world!\").offsets)\n",
    "print(tokenizer.encode(\"Hello, world!\").attention_mask)\n",
    "print(tokenizer.encode(\"Hello, world!\").special_tokens_mask)\n",
    "print(tokenizer.encode(\"Hello, world!\").overflowing)\n",
    "\n",
    "print(x_train[0][:1],\"\\n\",x_train[1][:1],\"\\n\",x_train[2][:1],)\n",
    "print(y_train[0][:1], \"\\n\",y_train[1][:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76e3d9-b3b2-4a19-a738-1ce6579abbcd",
   "metadata": {},
   "source": [
    "使用 `TFBertModel.from_pretrained()` 加载的 BERT 模型输出通常包含以下内容，具体取决于模型配置和输入参数。假设你使用的是 `BertModel`（即基础的 BERT 模型，不带任何任务头部，比如分类或问答），并且你提供了 `input_ids`、`token_type_ids` 和 `attention_mask`，则它的输出主要包括：\n",
    "\n",
    "### 1. **`last_hidden_state` (最后隐藏层状态)**:\n",
    "   - **类型**: 张量 (Tensor)，形状为 `(batch_size, sequence_length, hidden_size)`\n",
    "   - **描述**: 这是 BERT 模型的最后一层的输出。对于每一个输入 token，它提供了一个 `hidden_size` 维的向量表示。`sequence_length` 是输入序列的长度，`hidden_size` 是 BERT 模型的隐藏层大小，通常为 768（对于 `bert-base-uncased`）。\n",
    "   \n",
    "   **用途**: 你可以将这个输出用于进一步的下游任务，如文本分类、命名实体识别、序列标注等。每个 token 的输出向量捕捉了其上下文语义。\n",
    "\n",
    "### 2. **`pooler_output` (池化输出)**:\n",
    "   - **类型**: 张量 (Tensor)，形状为 `(batch_size, hidden_size)`\n",
    "   - **描述**: 这个是对句子级别表示的总结。它取的是 `[CLS]` token 的输出，并经过一个全连接层和激活函数（通常是 `tanh`）之后作为整个句子的表示向量。\n",
    "   \n",
    "   **用途**: 这个输出经常用于句子级别的任务，例如句子分类。它可以认为是对整个句子内容的一个全局表示。\n",
    "\n",
    "### 具体代码示例和输出结果说明：\n",
    "\n",
    "```python\n",
    "from transformers import TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# 假设模型已经下载到了本地目录\n",
    "model = TFBertModel.from_pretrained(\"D:/bert-base-uncased\")\n",
    "\n",
    "# 输入数据示例\n",
    "input_ids = tf.constant([[101, 7592, 1010, 2129, 2024, 2017, 102]])  # 输入序列的 token ids\n",
    "token_type_ids = tf.constant([[0, 0, 0, 0, 0, 0, 0]])  # token 类型 ids，通常用于句子对任务\n",
    "attention_mask = tf.constant([[1, 1, 1, 1, 1, 1, 1]])  # 注意力掩码，表示哪些 token 需要关注\n",
    "\n",
    "# 模型前向传播，得到输出\n",
    "outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "\n",
    "# 查看输出\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output\n",
    "```\n",
    "\n",
    "### 输出内容:\n",
    "\n",
    "1. **`last_hidden_state`**:\n",
    "   - 形状: `(batch_size, sequence_length, hidden_size)`，即 `(1, 7, 768)`，因为输入序列长度是 7。\n",
    "   - 内容: 这是每个 token 的隐藏状态。输出的每个元素是该 token 的上下文表示。\n",
    "\n",
    "2. **`pooler_output`**:\n",
    "   - 形状: `(batch_size, hidden_size)`，即 `(1, 768)`。\n",
    "   - 内容: 这是 `[CLS]` token 的隐藏状态经过池化后的表示，是整个输入序列的全局表示。\n",
    "\n",
    "### 输出总结:\n",
    "- **`last_hidden_state`** 包含每个输入 token 在经过 BERT 模型后的上下文表示，是用于序列标注等任务的主要输出。\n",
    "- **`pooler_output`** 则是对句子进行全局表示的输出，常用于句子级任务，如分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b57828d-cf5b-4803-9a9f-80d9b704207e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at D:/bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at D:/bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    ## BERT encoder\n",
    "    encoder = TFBertModel.from_pretrained(\"D:/bert-base-uncased\")\n",
    "\n",
    "    ## QA Model\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    embedding = encoder(\n",
    "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "    )[0]\n",
    "\n",
    "    start_logits = tf.keras.layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
    "    start_logits = tf.keras.layers.Flatten()(start_logits)\n",
    "\n",
    "    end_logits = tf.keras.layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
    "    end_logits = tf.keras.layers.Flatten()(end_logits)\n",
    "\n",
    "    start_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(start_logits)\n",
    "    end_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(end_logits)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[start_probs, end_probs],\n",
    "    )\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97091e41-3509-4294-af20-55e57a5308fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    exclude = set(string.punctuation)\n",
    "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    text = re.sub(regex, \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "class ExactMatch(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "\n",
    "            normalized_pred_ans = normalize_text(pred_ans)\n",
    "            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6cd901-f6fb-4828-997b-393e319c0245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "323/323 [==============================] - 43s 128ms/step\n",
      "\n",
      "epoch=1, exact match score=0.75\n",
      "10767/10767 [==============================] - 1004s 93ms/step - loss: 2.5409 - activation_1_loss: 1.3308 - activation_2_loss: 1.2101\n",
      "Epoch 2/3\n",
      "323/323 [==============================] - 42s 129ms/step\n",
      "\n",
      "epoch=2, exact match score=0.75\n",
      "10767/10767 [==============================] - 1000s 93ms/step - loss: 1.7251 - activation_1_loss: 0.9139 - activation_2_loss: 0.8112\n",
      "Epoch 3/3\n",
      "323/323 [==============================] - 42s 130ms/step\n",
      "\n",
      "epoch=3, exact match score=0.74\n",
      "10767/10767 [==============================] - 1000s 93ms/step - loss: 1.3432 - activation_1_loss: 0.7190 - activation_2_loss: 0.6242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18e6411e530>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=8, callbacks=[exact_match_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b58d0fad-716a-4260-8708-4b6a3c18a1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 305ms/step\n",
      "预测：denver broncos，标签：denver broncos\n",
      "预测：carolina panthers，标签：carolina panthers\n",
      "预测：levis stadium，标签：santa clara california\n",
      "预测：denver broncos，标签：denver broncos\n",
      "预测：golden，标签：golden\n",
      "预测：arabic numerals，标签：golden anniversary\n",
      "预测：february 7 2016，标签：february 7 2016\n",
      "预测：american football conference，标签：american football conference\n",
      "预测：arabic numerals，标签：golden anniversary\n",
      "预测：american football conference，标签：american football conference\n"
     ]
    }
   ],
   "source": [
    "def test(n):\n",
    "    test = [x_eval[0][:n], x_eval[1][:n], x_eval[2][:n]]\n",
    "    pred_start, pred_end = model.predict(test)\n",
    "    eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "    pred_result = []\n",
    "    true_result = []\n",
    "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "        squad_eg = eval_examples_no_skip[idx]\n",
    "        offsets = squad_eg.context_token_to_char\n",
    "        start = np.argmax(start)\n",
    "        end = np.argmax(end)\n",
    "        if start >= len(offsets):\n",
    "            continue\n",
    "        pred_char_start = offsets[start][0]\n",
    "        if end < len(offsets):\n",
    "            pred_char_end = offsets[end][1]\n",
    "            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "        else:\n",
    "            pred_ans = squad_eg.context[pred_char_start:]\n",
    "    \n",
    "        normalized_pred_ans = normalize_text(pred_ans)\n",
    "        pred_result.append(normalized_pred_ans)\n",
    "        true_start = y_eval[0][idx]\n",
    "        true_end = y_eval[1][idx]\n",
    "        true_result.append(normalize_text(squad_eg.context[ offsets[true_start][0] :  offsets[true_end][1] ]))\n",
    "    return pred_result[:n], true_result[:n]\n",
    "for a,b in zip(*test(10)):\n",
    "    print(f\"预测：{a}，标签：{b}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd3e06-e1d8-4335-af0c-dda63785409f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8f196-41ed-4eca-aeca-dd8f2d65afdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2.10.0-py-3.10",
   "language": "python",
   "name": "tf-gpu-2.10.0-py-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
